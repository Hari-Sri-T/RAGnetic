{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52fe2108",
   "metadata": {},
   "source": [
    "### Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b82100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-docx in ./.venv/lib/python3.10/site-packages (1.2.0)\n",
      "Requirement already satisfied: PyMuPDF in ./.venv/lib/python3.10/site-packages (1.26.3)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement mailparser (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for mailparser\u001b[0m\u001b[31m\n",
      "\u001b[0mRequirement already satisfied: transformers in ./.venv/lib/python3.10/site-packages (4.55.0)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.10.0-py3-none-any.whl (374 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.7/374.7 KB\u001b[0m \u001b[31m733.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.10/site-packages (from transformers) (2025.7.34)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.10/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.10/site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.10/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.10/site-packages (from transformers) (0.6.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.10/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./.venv/lib/python3.10/site-packages (from transformers) (0.34.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in ./.venv/lib/python3.10/site-packages (from accelerate) (2.8.0)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.10/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (1.13.1.3)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: triton==3.4.0 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.3.3.83)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in ./.venv/lib/python3.10/site-packages (from triton==3.4.0->torch>=2.0.0->accelerate) (59.6.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests->transformers) (2025.8.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Installing collected packages: accelerate\n",
      "Successfully installed accelerate-1.10.0\n",
      "zsh:1: no matches found: sentence-transformers[torch]\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement cross-encoder (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for cross-encoder\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Use your environment manager; example with pip\n",
    "!pip install python-docx PyMuPDF mail-parser langchain faiss-cpu sentence-transformers rank_bm25\n",
    "!pip install transformers accelerate\n",
    "!pip install sentence-transformers[torch]  # if needed\n",
    "!pip install cross-encoder  # or use sentence-transformers CrossEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "60c66993",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-docx in ./.venv/lib/python3.10/site-packages (1.2.0)\n",
      "Requirement already satisfied: PyMuPDF in ./.venv/lib/python3.10/site-packages (1.26.3)\n",
      "Requirement already satisfied: mail-parser in ./.venv/lib/python3.10/site-packages (4.1.4)\n",
      "Requirement already satisfied: sentence-transformers in ./.venv/lib/python3.10/site-packages (5.1.0)\n",
      "Requirement already satisfied: faiss-cpu in ./.venv/lib/python3.10/site-packages (1.11.0.post1)\n",
      "Collecting rank_bm25\n",
      "  Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
      "Requirement already satisfied: typing_extensions>=4.9.0 in ./.venv/lib/python3.10/site-packages (from python-docx) (4.14.1)\n",
      "Requirement already satisfied: lxml>=3.1.0 in ./.venv/lib/python3.10/site-packages (from python-docx) (6.0.0)\n",
      "Requirement already satisfied: six in ./.venv/lib/python3.10/site-packages (from mail-parser) (1.17.0)\n",
      "Requirement already satisfied: scipy in ./.venv/lib/python3.10/site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: torch>=1.11.0 in ./.venv/lib/python3.10/site-packages (from sentence-transformers) (2.8.0)\n",
      "Requirement already satisfied: scikit-learn in ./.venv/lib/python3.10/site-packages (from sentence-transformers) (1.7.1)\n",
      "Requirement already satisfied: Pillow in ./.venv/lib/python3.10/site-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in ./.venv/lib/python3.10/site-packages (from sentence-transformers) (4.55.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in ./.venv/lib/python3.10/site-packages (from sentence-transformers) (0.34.3)\n",
      "Requirement already satisfied: tqdm in ./.venv/lib/python3.10/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.10/site-packages (from faiss-cpu) (25.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in ./.venv/lib/python3.10/site-packages (from faiss-cpu) (2.2.6)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.1.7)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.7.0)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.18.0)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./.venv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in ./.venv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./.venv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.4.1)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./.venv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./.venv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1.3)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./.venv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./.venv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./.venv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./.venv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./.venv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.90)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in ./.venv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./.venv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./.venv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (10.3.9.90)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./.venv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./.venv/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (12.5.8.93)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in ./.venv/lib/python3.10/site-packages (from triton==3.4.0->torch>=1.11.0->sentence-transformers) (59.6.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.1)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.7.34)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in ./.venv/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.8.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Installing collected packages: rank_bm25\n",
      "Successfully installed rank_bm25-0.2.2\n",
      "Requirement already satisfied: transformers in ./.venv/lib/python3.10/site-packages (4.55.0)\n",
      "Requirement already satisfied: accelerate in ./.venv/lib/python3.10/site-packages (1.10.0)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in ./.venv/lib/python3.10/site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./.venv/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in ./.venv/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.venv/lib/python3.10/site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in ./.venv/lib/python3.10/site-packages (from transformers) (0.34.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in ./.venv/lib/python3.10/site-packages (from transformers) (2025.7.34)\n",
      "Requirement already satisfied: filelock in ./.venv/lib/python3.10/site-packages (from transformers) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in ./.venv/lib/python3.10/site-packages (from transformers) (2.2.6)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in ./.venv/lib/python3.10/site-packages (from transformers) (0.6.1)\n",
      "Requirement already satisfied: requests in ./.venv/lib/python3.10/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: torch>=2.0.0 in ./.venv/lib/python3.10/site-packages (from accelerate) (2.8.0)\n",
      "Requirement already satisfied: psutil in ./.venv/lib/python3.10/site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.7.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in ./.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./.venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (2.27.3)\n",
      "Requirement already satisfied: sympy>=1.13.3 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.8.90 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.9.90 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (10.3.9.90)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.7.3.90 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.7.3.90)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.8.93 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: jinja2 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: triton==3.4.0 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.4.0)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (0.7.1)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12==12.8.93 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.8.93)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.3.3.83 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (11.3.3.83)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.8.90 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.5.8.93 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.5.8.93)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.8.90 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.8.90)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.8.4.1 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (12.8.4.1)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (9.10.2.21)\n",
      "Requirement already satisfied: nvidia-cufile-cu12==1.13.1.3 in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (1.13.1.3)\n",
      "Requirement already satisfied: networkx in ./.venv/lib/python3.10/site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: setuptools>=40.8.0 in ./.venv/lib/python3.10/site-packages (from triton==3.4.0->torch>=2.0.0->accelerate) (59.6.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.venv/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.venv/lib/python3.10/site-packages (from requests->transformers) (2025.8.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.venv/lib/python3.10/site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in ./.venv/lib/python3.10/site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in ./.venv/lib/python3.10/site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.venv/lib/python3.10/site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "# Use your environment manager; example with pip\n",
    "!pip install python-docx PyMuPDF mail-parser sentence-transformers 'faiss-cpu' rank_bm25\n",
    "!pip install transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "251ac738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ollama\n",
      "  Downloading ollama-0.5.3-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: httpx>=0.27 in ./.venv/lib/python3.10/site-packages (from ollama) (0.28.1)\n",
      "Requirement already satisfied: pydantic>=2.9 in ./.venv/lib/python3.10/site-packages (from ollama) (2.11.7)\n",
      "Requirement already satisfied: httpcore==1.* in ./.venv/lib/python3.10/site-packages (from httpx>=0.27->ollama) (1.0.9)\n",
      "Requirement already satisfied: idna in ./.venv/lib/python3.10/site-packages (from httpx>=0.27->ollama) (3.10)\n",
      "Requirement already satisfied: anyio in ./.venv/lib/python3.10/site-packages (from httpx>=0.27->ollama) (4.10.0)\n",
      "Requirement already satisfied: certifi in ./.venv/lib/python3.10/site-packages (from httpx>=0.27->ollama) (2025.8.3)\n",
      "Requirement already satisfied: h11>=0.16 in ./.venv/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=4.12.2 in ./.venv/lib/python3.10/site-packages (from pydantic>=2.9->ollama) (4.14.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./.venv/lib/python3.10/site-packages (from pydantic>=2.9->ollama) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in ./.venv/lib/python3.10/site-packages (from pydantic>=2.9->ollama) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in ./.venv/lib/python3.10/site-packages (from pydantic>=2.9->ollama) (0.4.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in ./.venv/lib/python3.10/site-packages (from anyio->httpx>=0.27->ollama) (1.3.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./.venv/lib/python3.10/site-packages (from anyio->httpx>=0.27->ollama) (1.3.0)\n",
      "Installing collected packages: ollama\n",
      "Successfully installed ollama-0.5.3\n"
     ]
    }
   ],
   "source": [
    "!pip install ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f85fd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mail-parser\n",
      "  Downloading mail_parser-4.1.4-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: six in ./.venv/lib/python3.10/site-packages (from mail-parser) (1.17.0)\n",
      "Installing collected packages: mail-parser\n",
      "Successfully installed mail-parser-4.1.4\n"
     ]
    }
   ],
   "source": [
    "!pip install mail-parser  # if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3eb1b90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement cross-encoder (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for cross-encoder\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install cross-encoder  # or use sentence-transformers CrossEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "704e4350",
   "metadata": {},
   "source": [
    "### Document Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "050a3616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "import docx\n",
    "import mailparser\n",
    "import re\n",
    "from typing import List, Dict\n",
    "\n",
    "def extract_pdf_with_structure(pdf_path: str) -> List[Dict]:\n",
    "    \"\"\"Returns list of {'text': str, 'page': int, 'block': int, 'heading': str or None}\"\"\"\n",
    "    doc = fitz.open(pdf_path)\n",
    "    results = []\n",
    "    for pageno in range(doc.page_count):\n",
    "        page = doc.load_page(pageno)\n",
    "        blocks = page.get_text(\"dict\")[\"blocks\"]\n",
    "        for b_idx, block in enumerate(blocks):\n",
    "            # block may contain lines; join them\n",
    "            lines = []\n",
    "            for line in block.get(\"lines\", []):\n",
    "                for span in line.get(\"spans\", []):\n",
    "                    lines.append(span.get(\"text\", \"\"))\n",
    "            text = \" \".join(lines).strip()\n",
    "            if not text:\n",
    "                continue\n",
    "            # Simple heuristic detect heading (all caps or ends with ':')\n",
    "            heading = None\n",
    "            if len(text) < 120 and (text.isupper() or text.endswith(\":\")):\n",
    "                heading = text\n",
    "            results.append({\"text\": text, \"page\": pageno+1, \"block\": b_idx, \"heading\": heading})\n",
    "    return results\n",
    "\n",
    "def extract_docx_with_structure(docx_path: str):\n",
    "    doc = docx.Document(docx_path)\n",
    "    results = []\n",
    "    for i, para in enumerate(doc.paragraphs):\n",
    "        text = para.text.strip()\n",
    "        if not text:\n",
    "            continue\n",
    "        # heading detection via style or bold\n",
    "        style = para.style.name.lower() if para.style else \"\"\n",
    "        heading = text if \"heading\" in style else None\n",
    "        results.append({\"text\": text, \"para_idx\": i, \"heading\": heading})\n",
    "    return results\n",
    "\n",
    "def extract_email(msg_path: str):\n",
    "    m = mailparser.parse_from_file(msg_path)\n",
    "    body = m.body or \"\"\n",
    "    # optionally parse attachments separately\n",
    "    return [{\"text\": body, \"from\": m.from_, \"to\": m.to, \"subject\": m.subject}]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d99b8fb",
   "metadata": {},
   "source": [
    "### Clause Aware Sematic Searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2ae0d450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "CLAUSE_RE = re.compile(r'^\\s*\\d+(\\.\\d+){0,}\\s+')  # detects \"1.\", \"2.1\", etc.\n",
    "\n",
    "def semantic_chunker(structured_parts, max_tokens=350, overlap_tokens=50):\n",
    "    \"\"\"\n",
    "    structured_parts: list of dicts from extract_pdf_with_structure\n",
    "    returns list of chunks: dicts {text, metadata}\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    buffer = \"\"\n",
    "    buffer_meta = {\"pages\": set(), \"headings\": [], \"source_blocks\": []}\n",
    "\n",
    "    def flush_buffer():\n",
    "        nonlocal buffer, buffer_meta\n",
    "        if not buffer.strip(): \n",
    "            return\n",
    "        chunks.append({\n",
    "            \"text\": buffer.strip(),\n",
    "            \"pages\": sorted(buffer_meta[\"pages\"]),\n",
    "            \"headings\": buffer_meta[\"headings\"],\n",
    "            \"sources\": buffer_meta[\"source_blocks\"]\n",
    "        })\n",
    "        # set overlap: keep last `overlap_tokens` tokens as new buffer\n",
    "        toks = tokenizer.encode(buffer)\n",
    "        overlap_toks = toks[-overlap_tokens:] if len(toks) > overlap_tokens else toks\n",
    "        buffer = tokenizer.decode(overlap_toks) if overlap_toks else \"\"\n",
    "        buffer_meta = {\"pages\": set(), \"headings\": [], \"source_blocks\": []}\n",
    "\n",
    "    for part in structured_parts:\n",
    "        text = part[\"text\"]\n",
    "        # if heading or clause start: prefer flush (start new chunk)\n",
    "        if part.get(\"heading\") or CLAUSE_RE.match(text):\n",
    "            # flush current chunk if not empty\n",
    "            if buffer.strip():\n",
    "                flush_buffer()\n",
    "            # start new chunk with the heading/clause\n",
    "            buffer += (text + \"\\n\")\n",
    "            buffer_meta[\"pages\"].add(part.get(\"page\", part.get(\"para_idx\", 0)))\n",
    "            buffer_meta[\"headings\"].append(part.get(\"heading\") or \"\")\n",
    "            buffer_meta[\"source_blocks\"].append((part.get(\"page\",0), part.get(\"block\",0)))\n",
    "            # if this too large, flush\n",
    "            if len(tokenizer.encode(buffer)) > max_tokens:\n",
    "                flush_buffer()\n",
    "            continue\n",
    "\n",
    "        # otherwise append\n",
    "        buffer += (\" \" + text)\n",
    "        buffer_meta[\"pages\"].add(part.get(\"page\", part.get(\"para_idx\", 0)))\n",
    "        buffer_meta[\"source_blocks\"].append((part.get(\"page\",0), part.get(\"block\",0)))\n",
    "        if len(tokenizer.encode(buffer)) > max_tokens:\n",
    "            flush_buffer()\n",
    "\n",
    "    if buffer.strip():\n",
    "        flush_buffer()\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4132c705",
   "metadata": {},
   "source": [
    "### D — Embeddings: high-quality & normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83fcc0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "def embed_texts(texts: List[str]):\n",
    "    embs = model.encode(texts, show_progress_bar=True, convert_to_numpy=True)\n",
    "    # L2-normalize for cosine in FAISS\n",
    "    norms = np.linalg.norm(embs, axis=1, keepdims=True)\n",
    "    norms[norms==0] = 1.0\n",
    "    embs = embs / norms\n",
    "    return embs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f85e6b51",
   "metadata": {},
   "source": [
    "### E — Vector DB (FAISS) with metadata mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1f01374e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import json\n",
    "\n",
    "def build_faiss_index(embs: np.ndarray, ids: List[int], ef_construction=200, M=64):\n",
    "    d = embs.shape[1]\n",
    "    # HNSW index\n",
    "    index = faiss.IndexHNSWFlat(d, M)\n",
    "    index.hnsw.efConstruction = ef_construction\n",
    "    # wrap in ID map so we can assign stable ids\n",
    "    id_index = faiss.IndexIDMap(index)\n",
    "    id_index.add_with_ids(embs, np.array(ids, dtype='int64'))\n",
    "    return id_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6185fa5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (676 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created 62 chunks.\n"
     ]
    }
   ],
   "source": [
    "# 1. Ingest a document to get structured parts\n",
    "# Make sure you have a file named 'policy.pdf' or change the name\n",
    "structured = extract_pdf_with_structure(\"policy.pdf\") \n",
    "\n",
    "# 2. Run the semantic chunker to create the 'chunks' variable\n",
    "chunks = semantic_chunker(structured)\n",
    "\n",
    "# NOW you can run the code that uses the 'chunks' variable\n",
    "print(f\"Successfully created {len(chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b39b2a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# metadata store\n",
    "metadata_store = {}  # id -> {text, pages, headings, source}\n",
    "# when constructing:\n",
    "for i, chunk in enumerate(chunks):\n",
    "    id_ = i+1\n",
    "    metadata_store[id_] = chunk\n",
    "# save metadata_store to disk\n",
    "with open(\"meta.json\", \"w\") as f:\n",
    "    json.dump(metadata_store, f, ensure_ascii=False, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac02d03c",
   "metadata": {},
   "source": [
    "### F — Hybrid retrieval: BM25 + Embeddings + union + rerank"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88cf250",
   "metadata": {},
   "source": [
    "#### 1) BM25 (exact-match for clauses & numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "71d5df36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from rank_bm25 import BM25Okapi\n",
    "tokenized_corpus = [tokenizer.tokenize(c['text']) for c in chunks]\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "def bm25_search(query, top_k=10):\n",
    "    tokenized_q = tokenizer.tokenize(query)\n",
    "    scores = bm25.get_scores(tokenized_q)\n",
    "    top_idxs = np.argsort(scores)[::-1][:top_k]\n",
    "    return list(top_idxs), scores[top_idxs]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffee1a4",
   "metadata": {},
   "source": [
    "#### 2) Embedding search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ddbe5b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_search(query, faiss_index, top_k=10):\n",
    "    q_emb = embed_texts([query])  # normalized\n",
    "    D, I = faiss_index.search(q_emb.astype('float32'), top_k)\n",
    "    return I[0].tolist(), D[0].tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdd9577",
   "metadata": {},
   "source": [
    "### 3) Combine results and rerank using CrossEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bcb8b1c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import CrossEncoder\n",
    "reranker = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")  # good speed/quality\n",
    "\n",
    "def hybrid_retrieve(query, top_k=8):\n",
    "    # Retrieve a smaller number of initial candidates, e.g., 15 from each search\n",
    "    bm25_ids, _ = bm25_search(query, top_k=15)\n",
    "    emb_ids, _ = embed_search(query, faiss_index, top_k=15)\n",
    "\n",
    "    # Combine and deduplicate the candidates\n",
    "    candidate_ids = list(dict.fromkeys(bm25_ids + emb_ids)) \n",
    "    candidate_texts = [metadata_store[int(cid)+1]['text'] for cid in candidate_ids]\n",
    "\n",
    "    # The reranker now has fewer documents to process\n",
    "    pairs = [(query, t) for t in candidate_texts]\n",
    "    scores = reranker.predict(pairs)\n",
    "    \n",
    "    ranked = sorted(zip(candidate_ids, candidate_texts, scores), key=lambda x: x[2], reverse=True)\n",
    "    \n",
    "    # Return the final top_k results after reranking\n",
    "    top_ranked = ranked[:top_k] \n",
    "    return top_ranked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a0ee84",
   "metadata": {},
   "source": [
    "### Answer Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d126539d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_answer(query, top_chunks, llm_call_fn):\n",
    "    \"\"\"\n",
    "    top_chunks: list of tuples (id, text, rerank_score)\n",
    "    llm_call_fn: function that accepts prompt and returns answer & score\n",
    "    \"\"\"\n",
    "    # build prompt with sources and instructions\n",
    "    sources_text = \"\\n\\n\".join([f\"[Source {i+1} | id={cid} | score={round(s,3)}]\\n{txt}\" \n",
    "                                for i, (cid, txt, s) in enumerate(top_chunks)])\n",
    "    prompt = f\"\"\"\n",
    "You are a policy/contract assistant. Use ONLY the information in the provided sources to answer the query.\n",
    "If answer is not in sources, say \"Not found in documents\".\n",
    "Provide:\n",
    "1) Short answer (1-3 sentences)\n",
    "2) Supporting quotes with source ids and pages\n",
    "3) Explanation of why these sources match (2-3 lines)\n",
    "4) Confidence (0-1)\n",
    "\n",
    "SOURCES:\n",
    "{sources_text}\n",
    "\n",
    "QUERY: {query}\n",
    "\n",
    "Answer in JSON only with keys: answer, evidence, explanation, confidence\n",
    "\"\"\"\n",
    "    response = llm_call_fn(prompt)  # implement with OpenAI/LLM of choice\n",
    "    # parse response if LLM already returns JSON, else parse\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f655d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import json\n",
    "\n",
    "# Let's assume you are running Phi-3 Mini locally via 'ollama run phi-3:mini'\n",
    "OLLAMA_MODEL = 'phi-3:mini' \n",
    "\n",
    "def llm_call_fn(prompt: str):\n",
    "    \"\"\"\n",
    "    Function that accepts a prompt and returns a structured answer from a local Ollama model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # The 'format=\"json\"' argument tells Ollama to guarantee the output is valid JSON\n",
    "        response = ollama.chat(\n",
    "            model=OLLAMA_MODEL,\n",
    "            messages=[{'role': 'user', 'content': prompt}],\n",
    "            format='json'\n",
    "        )\n",
    "        \n",
    "        # The response content should be a JSON string, so we parse it into a dict\n",
    "        response_content = response['message']['content']\n",
    "        return json.loads(response_content)\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle potential errors (e.g., model not running, JSON parsing failed)\n",
    "        print(f\"Error during LLM call: {e}\")\n",
    "        return {\n",
    "            \"answer\": \"Error generating response from the model.\",\n",
    "            \"evidence\": [],\n",
    "            \"explanation\": \"\",\n",
    "            \"confidence\": 0.0\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b530924f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import json\n",
    "\n",
    "# Use the Nous-Hermes 2 model you have already downloaded\n",
    "OLLAMA_MODEL = 'nous-hermes2' \n",
    "\n",
    "def llm_call_fn(prompt: str):\n",
    "    \"\"\"\n",
    "    Function that accepts a prompt and returns a structured answer from a local Ollama model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # The 'format=\"json\"' argument tells Ollama to guarantee the output is valid JSON.\n",
    "        response = ollama.chat(\n",
    "            model=OLLAMA_MODEL,\n",
    "            messages=[{'role': 'user', 'content': prompt}],\n",
    "            format='json'\n",
    "        )\n",
    "        \n",
    "        # The response content should be a JSON string, so we parse it into a dict\n",
    "        response_content = response['message']['content']\n",
    "        return json.loads(response_content)\n",
    "\n",
    "    except Exception as e:\n",
    "        # Handle potential errors (e.g., model not running, JSON parsing failed)\n",
    "        print(f\"Error during LLM call: {e}\")\n",
    "        return {\n",
    "            \"answer\": \"Error generating response from the model.\",\n",
    "            \"evidence\": [],\n",
    "            \"explanation\": \"\",\n",
    "            \"confidence\": 0.0\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3f8d3de",
   "metadata": {},
   "source": [
    "# Final Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640e7824",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 2/2 [00:01<00:00,  1.32it/s]\n",
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 109.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answer': '30 days', 'evidence': [{'source_id': 44, 'page': 7, 'quote': 'In case of Premium payment in instalments, if the due instalment premium is paid within Grace Period during the Policy Period, coverage shall be available during the Grace Period.'}], 'explanation': 'From source 44, page 7, it is clear that for premium payments made in instalments, there is a grace period of 30 days.', 'confidence': 1}\n"
     ]
    }
   ],
   "source": [
    "# 1. Ingest -> structured_parts\n",
    "structured = extract_pdf_with_structure(\"policy.pdf\")    # or docx/email functions\n",
    "\n",
    "# 2. Chunk\n",
    "chunks = semantic_chunker(structured)\n",
    "\n",
    "# 3. Embed and build FAISS\n",
    "texts = [c[\"text\"] for c in chunks]\n",
    "embs = embed_texts(texts).astype('float32')\n",
    "ids = list(range(1, len(texts)+1))\n",
    "faiss_index = build_faiss_index(embs, ids)\n",
    "# store metadata_store as id->chunk\n",
    "\n",
    "# 4. Build BM25\n",
    "tokenized_corpus = [tokenizer.tokenize(t) for t in texts]\n",
    "bm25 = BM25Okapi(tokenized_corpus)\n",
    "\n",
    "# 5. Query -> hybrid_retrieve -> top_chunks\n",
    "top_chunks = hybrid_retrieve(\"What is the grace period for premium payment under the National Parivar Mediclaim Plus Policy?\", top_k=5)\n",
    "print\n",
    "# 6. Generate answer\n",
    "resp_json = generate_answer(\"What is the grace period for premium payment under the National Parivar Mediclaim Plus Policy?\", top_chunks, llm_call_fn)\n",
    "\n",
    "print(resp_json)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
